{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to kal_utils","text":"<p>Kaleidoo utils package</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://BarLanderK.github.io/kal_utils</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v2083-date","title":"v2.0.8.3 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/BarLanderK/kal-utils/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>kal-utils could always use more documentation, whether as part of the official kal-utils docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/BarLanderK/kal-utils/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up kal-utils for local development.</p> <ol> <li> <p>Fork the kal-utils repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/kal-utils.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv kal-utils\n$ cd kal-utils/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 kal-utils tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/BarLanderK/kal-utils/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"handle_response/","title":"handle_response module","text":""},{"location":"helper/","title":"helper module","text":""},{"location":"helper/#kal_utils.helper.validate_dict_structure","title":"<code>validate_dict_structure(data, expected_data, check_extra_keys=False)</code>","text":"<p>Validate that all expected data is present and not empty, and optionally check for extra keys.</p> <ul> <li>data (dict): A dictionary containing the actual data to validate.</li> <li>expected_data (dict): A dictionary containing the expected data structure.</li> <li>check_extra_keys (bool): A boolean flag indicating whether to check for extra keys in the data.                            Default is False.</li> </ul> <ul> <li>bool: True if validation passes, False otherwise.</li> </ul> Source code in <code>kal_utils/helper.py</code> <pre><code>def validate_dict_structure(data: dict, expected_data: dict, check_extra_keys: bool = False) -&gt; bool:\n    \"\"\"\n    Validate that all expected data is present and not empty, and optionally check for extra keys.\n\n    Parameters:\n    - data (dict): A dictionary containing the actual data to validate.\n    - expected_data (dict): A dictionary containing the expected data structure.\n    - check_extra_keys (bool): A boolean flag indicating whether to check for extra keys in the data.\n                               Default is False.\n\n    Returns:\n    - bool: True if validation passes, False otherwise.\n    \"\"\"\n    # Check for missing or empty values\n    for key in expected_data.keys():\n        if not data.get(key):\n            return False\n\n    if check_extra_keys:\n        # Check for extra keys\n        for key in data.keys():\n            if key not in expected_data:\n                return False\n\n    return True\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install kal-utils, run this command in your terminal:</p> <pre><code>pip install kal-utils\n</code></pre> <p>This is the preferred method to install kal-utils, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install kal-utils from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/BarLanderK/kal-utils\n</code></pre>"},{"location":"kal_utils/","title":"kal_utils module","text":"<p>Main module.</p>"},{"location":"logger/","title":"logger module","text":""},{"location":"logger/#kal_utils.logger.JsonFormatter","title":"<code> JsonFormatter            (Formatter)         </code>","text":"Source code in <code>kal_utils/logger.py</code> <pre><code>class JsonFormatter(logging.Formatter):\n    def format(self, record):\n        log_message = {\n            \"time\": self.formatTime(record, self.datefmt),\n            \"name\": record.name,\n            \"level\": record.levelname,\n            \"message\": record.getMessage()\n        }\n        return json.dumps(log_message)\n</code></pre>"},{"location":"logger/#kal_utils.logger.JsonFormatter.format","title":"<code>format(self, record)</code>","text":"<p>Format the specified record as text.</p> <p>The record's attribute dictionary is used as the operand to a string formatting operation which yields the returned string. Before formatting the dictionary, a couple of preparatory steps are carried out. The message attribute of the record is computed using LogRecord.getMessage(). If the formatting string uses the time (as determined by a call to usesTime(), formatTime() is called to format the event time. If there is exception information, it is formatted using formatException() and appended to the message.</p> Source code in <code>kal_utils/logger.py</code> <pre><code>def format(self, record):\n    log_message = {\n        \"time\": self.formatTime(record, self.datefmt),\n        \"name\": record.name,\n        \"level\": record.levelname,\n        \"message\": record.getMessage()\n    }\n    return json.dumps(log_message)\n</code></pre>"},{"location":"mongodb/","title":"mongodb module","text":""},{"location":"requests/","title":"requests module","text":""},{"location":"sorst/","title":"sorts module","text":""},{"location":"storage/","title":"Storage Module","text":"<p>This module provides an abstraction layer for working with different storage backends.</p>"},{"location":"storage/#classes","title":"Classes","text":""},{"location":"storage/#basestorage","title":"BaseStorage","text":"Source code in <code>kal_utils/storage/base_storage.py</code> <pre><code>class BaseStorage(ABC):\n    @abstractmethod\n    def create_bucket(self, bucket_name, location = \"\", storage_class = \"\"):\n        pass\n\n    @abstractmethod\n    def delete_bucket(self, bucket_name):\n        pass\n\n    @abstractmethod\n    def list_files(self, bucket_name, prefix=None):\n        pass\n\n    @abstractmethod\n    def get_file_metadata(self, bucket_name, file_path):\n        pass\n\n    @abstractmethod\n    def read_file(self, bucket_name: str, file_path: str) -&gt; str:\n        pass\n\n    @abstractmethod\n    def copy_file(self, bucket_name, source_file_path, destination_file_path):\n        pass\n\n    @abstractmethod\n    def rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name):\n        pass\n\n    @abstractmethod\n    def get_bucket_metadata(self, bucket_name):\n        pass\n\n    @abstractmethod\n    def set_bucket_permissions(self, bucket_name, entity, role):\n        pass\n\n    @abstractmethod\n    def set_file_permissions(self, bucket_name, file_path, entity, role):\n        pass\n\n    @abstractmethod\n    def upload_to_bucket(self, bucket_name, file_stream, destination_blob_name, content_type='application/octet-stream'):\n        pass\n\n    @abstractmethod\n    def upload_from_local(self, bucket_name: str, file_path: str, destination_blob_name: str,\n                          content_type='application/octet-stream'):\n        pass\n\n    @abstractmethod\n    def move_folder(self, bucket_name, source_folder, destination_folder):\n        pass\n\n    @abstractmethod\n    def move_file(self, bucket_name, source_file_path, destination_file_path):\n        pass\n\n    @abstractmethod\n    def delete_folder(self, bucket_name, folder_path):\n        pass\n\n    @abstractmethod\n    def delete_file(self, bucket_name, file_path):\n        pass\n\n    @abstractmethod\n    async def generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60):\n        pass\n\n    @abstractmethod\n    def list_buckets(self):\n        pass\n\n    @abstractmethod\n    def download_file(self, bucket_name, file_path, local_path=None):\n        pass\n</code></pre>"},{"location":"storage/#gcsstorage","title":"GCSStorage","text":"Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>class GCSStorage(BaseStorage):\n    def __init__(self, credentials_json=None):\n        if credentials_json:\n            if os.path.isfile(credentials_json):\n                logger.info(\"Using credentials from file path\")\n                with open(credentials_json, 'r') as f:\n                    service_account_info = json.load(f)\n            else:\n                try:\n                    logger.info(\"Trying to decode base64 credentials\")\n                    decoded_key = base64.b64decode(credentials_json).decode('utf-8')\n                    service_account_info = json.loads(decoded_key)\n                except (base64.binascii.Error, ValueError) as e:\n                    logger.error(\"Failed to decode base64 string, using it as JSON string\")\n                    service_account_info = json.loads(credentials_json)\n\n            credentials = service_account.Credentials.from_service_account_info(service_account_info)\n            self.storage_client = storage.Client(credentials=credentials)\n        else:\n            self.storage_client = storage.Client()\n\n    def create_bucket(self, bucket_name, location=\"me-west1\", storage_class=\"Standard\"):\n        try:\n            all_chars = string.ascii_letters + string.digits\n            new_name = bucket_name\n            while self.storage_client.lookup_bucket(new_name) is not None:\n                new_name += random.choice(all_chars)\n\n            bucket = self.storage_client.bucket(new_name)\n            bucket.location = location\n            bucket.storage_class = storage_class\n\n            bucket = self.storage_client.create_bucket(bucket)\n            logger.info(f'Bucket {new_name} created.')\n            return bucket\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to create bucket: {str(e)}\")\n            return None\n\n\n    def delete_bucket(self, bucket_name):\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            bucket.delete()\n            logger.info(f'Bucket {bucket_name} deleted.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete bucket: {str(e)}\")\n            return False\n\n    def list_files(self, bucket_name, prefix=None):\n        \"\"\"\n        Lists all files in a bucket or a specific folder in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            prefix (str, optional): The prefix (folder path) to list files from. Defaults to None.\n\n        Returns:\n            list: A list of file names in the specified bucket or folder.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            # List files in the bucket or a specific folder\n            blobs = bucket.list_blobs(prefix=prefix)\n            file_names = [blob.name for blob in blobs]\n            return file_names\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to list files: {str(e)}\")\n            return []\n\n    def get_file_metadata(self, bucket_name, file_path):\n        \"\"\"\n        Retrieves metadata for a specific file in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file to retrieve metadata for.\n\n        Returns:\n            dict: A dictionary containing the file's metadata.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            blob = bucket.get_blob(file_path)\n\n            if not blob:\n                logger.info(f'File {file_path} not found in bucket {bucket_name}.')\n                return {}\n\n            metadata = {\n                'name': blob.name,\n                'size': blob.size,\n                'content_type': blob.content_type,\n                'updated': blob.updated,\n                'generation': blob.generation,\n                'metageneration': blob.metageneration,\n                'md5_hash': blob.md5_hash,\n                'crc32c': blob.crc32c,\n                'etag': blob.etag,\n                'public_url': blob.public_url\n            }\n\n            return metadata\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to get file metadata: {str(e)}\")\n            return {}\n\n    def read_file(self, bucket_name: str, file_path: str) -&gt; str:\n        \"\"\"\n        Reads the content of a file from a Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The path to the file within the bucket.\n\n        Returns:\n            str: The content of the file as a string.\n\n        Raises:\n            Exception: If there's an error reading the file.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            blob = bucket.blob(file_path)\n            content = blob.download_as_text()\n            return content\n        except Exception as e:\n            logger.error(f\"Error reading file from GCS: {str(e)}\")\n            raise Exception(f\"Error reading file from GCS: {str(e)}\")\n\n    def copy_file(self, bucket_name, source_file_path, destination_file_path):\n        \"\"\"\n        Copies a file from one location to another within the same Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_path (str): The full path to the source file.\n            destination_file_path (str): The full path to the destination file.\n\n        Returns:\n            bool: True if the file was copied successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            source_blob = bucket.get_blob(source_file_path)\n\n            if not source_blob:\n                logger.info(f'Source file {source_file_path} not found in bucket {bucket_name}.')\n                return False\n\n            bucket.copy_blob(source_blob, bucket, destination_file_path)\n            logger.info(f'File {source_file_path} copied to {destination_file_path}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to copy file: {str(e)}\")\n            return False\n\n    def rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name):\n        \"\"\"\n        Renames a file in Google Cloud Storage by copying it to a new name and deleting the original.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_folder (str): The full path to the folder where the file is located.\n            source_file_name (str): The current file name.\n            new_file_name (str): The new file name.\n\n        Returns:\n            bool: True if the file was renamed successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            source_path = f\"{source_file_folder}/{source_file_name}\"\n            destination_path = f\"{source_file_folder}/{new_file_name}\"\n\n            source_blob = bucket.get_blob(source_path)\n            if not source_blob:\n                logger.error(f'Source file {source_path} not found in bucket {bucket_name}.')\n                return False\n\n            bucket.copy_blob(source_blob, bucket, destination_path)\n            logger.info(f'File {source_path} copied to {destination_path}.')\n\n            source_blob.delete()\n            logger.info(f'Source file {source_path} deleted.')\n\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to rename file: {str(e)}\")\n            return False\n\n    def get_bucket_metadata(self, bucket_name):\n        \"\"\"\n        Retrieves metadata for a specific bucket in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket to retrieve metadata for.\n\n        Returns:\n            dict: A dictionary containing the bucket's metadata.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            metadata = {\n                'id': bucket.id,\n                'name': bucket.name,\n                'location': bucket.location,\n                'storage_class': bucket.storage_class,\n                'created': bucket.time_created,\n                'updated': bucket.updated,\n                'default_event_based_hold': bucket.default_event_based_hold,\n                'retention_period': bucket.retention_period,\n                'labels': bucket.labels,\n                'versioning_enabled': bucket.versioning_enabled,\n                'cors': bucket.cors,\n                'lifecycle_rules': bucket.lifecycle_rules,\n                'logging': bucket.logging,\n                'encryption': bucket.encryption,\n                'owner': bucket.owner,\n                'acl': bucket.acl,\n                'default_acl': bucket.default_object_acl,\n            }\n\n            return metadata\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to get bucket metadata: {str(e)}\")\n            return {}\n\n    def set_bucket_permissions(self, bucket_name, entity, role):\n        \"\"\"\n        Sets permissions for a bucket in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n            role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n        Returns:\n            bool: True if the permissions were set successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # Get the bucket's ACL\n            acl = bucket.acl\n\n            # Clear existing ACLs for the entity\n            acl.revoke_entity(entity)\n\n            # Add the new permission\n            acl.entity_from_dict({'entity': entity, 'role': role})\n\n            # Save the changes to the ACL\n            acl.save()\n\n            logger.info(f'Permissions for entity {entity} set to {role} on bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to set bucket permissions: {str(e)}\")\n            return False\n\n    def set_file_permissions(self, bucket_name, file_path, entity, role):\n        \"\"\"\n        Sets permissions for a specific file in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file within the bucket.\n            entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n            role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n        Returns:\n            bool: True if the permissions were set successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            blob = bucket.get_blob(file_path)\n\n            if not blob:\n                logger.error(f'File {file_path} not found in bucket {bucket_name}.')\n                return False\n\n            # Get the blob's ACL\n            acl = blob.acl\n\n            # Clear existing ACLs for the entity\n            acl.revoke_entity(entity)\n\n            # Add the new permission\n            acl.entity_from_dict({'entity': entity, 'role': role})\n\n            # Save the changes to the ACL\n            acl.save()\n\n            logger.info(f'Permissions set successfully on file {file_path} in bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to set file permissions: {str(e)}\")\n            return False\n\n    def upload_to_bucket(self, bucket_name, file_stream, destination_blob_name,\n                         content_type='application/octet-stream'):\n        \"\"\"\n        Uploads a file to a bucket in Google Cloud Storage using chunked upload.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_stream (BytesIO): The byte stream of the file to upload.\n            destination_blob_name (str): The destination path and file name in the bucket.\n            content_type (str): The content type of the file (default: 'application/octet-stream').\n\n        Returns:\n            tuple: (bool, str) - (True if the file was uploaded successfully, False otherwise;\n                                  The public URL of the uploaded file if successful, None otherwise)\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            blob = bucket.blob(destination_blob_name)\n\n            chunk_size = 256 * 1024  # 256 KB chunks\n            file_stream.seek(0, 2)\n            file_size = file_stream.tell()\n            file_stream.seek(0)\n\n            with blob.open('wb') as f:\n                uploaded_bytes = 0\n                while True:\n                    chunk = file_stream.read(chunk_size)\n                    if not chunk:\n                        break\n                    f.write(chunk)\n                    uploaded_bytes += len(chunk)\n                    progress = (uploaded_bytes / file_size) * 100\n                    logger.info(f'Upload progress: {progress:.2f}%')\n\n            blob.content_type = content_type\n            blob.patch()\n\n            logger.info(f'File uploaded to {destination_blob_name} in bucket {bucket_name}.')\n            return True, blob.public_url\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to upload to bucket: {str(e)}\")\n            return False, None\n\n    def upload_from_local(self, bucket_name: str, file_path: str, destination_blob_name: str,\n                          content_type='application/octet-stream'):\n        \"\"\"Upload a file to the GCS bucket.\"\"\"\n        try:\n            # Get the bucket from GCS\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # Create a blob object with the destination name\n            blob = bucket.blob(destination_blob_name)\n\n            # Upload the file from the local path\n            blob.upload_from_filename(file_path, content_type=content_type)\n\n            logger.info(f\"File {file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n            return blob\n        except Exception as e:\n            logger.error(f\"Error while uploading to GCS: {str(e)}\", exc_info=True)\n            return None\n\n    def move_folder(self, bucket_name, source_folder, destination_folder):\n        \"\"\"\n        Moves all files from one folder to another within the same Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_folder (str): The path of the source folder.\n            destination_folder (str): The path of the destination folder.\n\n        Returns:\n            bool: True if the folder was moved successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # List all blobs in the source folder\n            blobs = list(bucket.list_blobs(prefix=source_folder))\n\n            # Move each blob to the destination folder\n            for blob in blobs:\n                new_name = blob.name.replace(source_folder, destination_folder, 1)\n                new_blob = bucket.rename_blob(blob, new_name)\n                logger.info(f'Moved {blob.name} to {new_blob.name}')\n\n            logger.info(f'All files moved from {source_folder} to {destination_folder} in bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to move folder: {str(e)}\")\n            return False\n\n    def move_file(self, bucket_name, source_file_path, destination_file_path):\n        \"\"\"\n        Moves a file from one location to another within the same Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_path (str): The full path to the source file.\n            destination_file_path (str): The full path to the destination file.\n\n        Returns:\n            tuple: (bool, str) - (True if the file was moved successfully, False otherwise;\n                                  The public URL of the moved file if successful, None otherwise)\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # Get the blob (file) object for the source file\n            source_blob = bucket.get_blob(source_file_path)\n\n            if not source_blob:\n                logger.error(f'Source file {source_file_path} not found in bucket {bucket_name}.')\n                return False, None\n\n            # Rename (move) the blob\n            new_blob = bucket.rename_blob(source_blob, destination_file_path)\n            logger.info(f'File {source_file_path} moved to {destination_file_path}.')\n\n            return True, new_blob.public_url\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to move file: {str(e)}\")\n            return False, None\n\n    def delete_folder(self, bucket_name, folder_path):\n        \"\"\"\n        Deletes all files in a folder within a Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            folder_path (str): The path of the folder to delete.\n\n        Returns:\n            bool: True if the folder was deleted successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # List all blobs in the folder\n            blobs = list(bucket.list_blobs(prefix=folder_path))\n\n            # Delete each blob in the folder\n            for blob in blobs:\n                blob.delete()\n                logger.info(f'File {blob.name} deleted.')\n\n            logger.info(f'All files in folder {folder_path} deleted from bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete folder: {str(e)}\")\n            return False\n\n    def delete_file(self, bucket_name, file_path):\n        \"\"\"\n        Deletes a file in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file, including folder(s) and file name.\n\n        Returns:\n            bool: True if the file was deleted successfully, False otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # Get the blob (file) object\n            blob = bucket.blob(file_path)\n\n            # Check if the blob exists\n            if not blob.exists():\n                logger.warning(f'File {file_path} not found in bucket {bucket_name}.')\n                return False\n\n            # Delete the file\n            blob.delete()\n\n            logger.info(f'File {file_path} deleted from bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete file: {str(e)}\")\n            return False\n\n    async def generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60):\n        \"\"\"\n        Generates a signed URL for a file in Google Cloud Storage.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file, including folder(s) and file name.\n            expiration_time_minutes (int): The time in minutes before the URL expires. Defaults to 60 minutes.\n\n        Returns:\n            str: The signed URL if generated successfully, None otherwise.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n\n            # Get the blob (file) object\n            blob = bucket.blob(file_path)\n\n            if not blob.exists():\n                logger.warning(f\"File {file_path} does not exist in bucket {bucket_name}\")\n                return None\n\n            # Generate a signed URL for the blob\n            url = blob.generate_signed_url(\n                version=\"v4\",\n                expiration=timedelta(minutes=expiration_time_minutes),\n                method=\"GET\"\n            )\n\n            return url\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to generate signed url: {str(e)}\")\n            return None\n\n    def list_buckets(self):\n        \"\"\"\n        Lists all buckets in the Google Cloud Storage project.\n\n        Returns:\n            list: A list of bucket names.\n        \"\"\"\n        try:\n            buckets = self.storage_client.list_buckets()\n\n            # Collect bucket names\n            bucket_names = [bucket.name for bucket in buckets]\n\n            return bucket_names\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to list buckets: {str(e)}\")\n            return []\n\n    def download_file(self, bucket_name, file_path, local_path=None):\n        \"\"\"\n        Downloads a file from a Google Cloud Storage bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file within the bucket.\n            local_path (str, optional): The local path where the file should be saved.\n                If not provided, the file content will be returned as bytes.\n\n        Returns:\n            Union[bytes, str, None]: If local_path is not provided, returns the file content as bytes.\n                If local_path is provided, returns the path to the saved file.\n                If some error occurs during the download, returns None.\n        \"\"\"\n        try:\n            bucket = self.storage_client.get_bucket(bucket_name)\n            blob = bucket.blob(file_path)\n\n            if not blob.exists():\n                logger.error(f\"File {file_path} not found in bucket {bucket_name}\")\n                return None\n\n            if local_path:\n                blob.download_to_filename(local_path)\n                logger.info(f\"File {file_path} downloaded to {local_path}\")\n                return local_path\n            else:\n                content = blob.download_as_bytes()\n                logger.info(f\"File {file_path} downloaded as bytes\")\n                return content\n\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to download file: {str(e)}\")\n            return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.copy_file","title":"<code>copy_file(self, bucket_name, source_file_path, destination_file_path)</code>","text":"<p>Copies a file from one location to another within the same Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_path</code> <code>str</code> <p>The full path to the source file.</p> required <code>destination_file_path</code> <code>str</code> <p>The full path to the destination file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was copied successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def copy_file(self, bucket_name, source_file_path, destination_file_path):\n    \"\"\"\n    Copies a file from one location to another within the same Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_path (str): The full path to the source file.\n        destination_file_path (str): The full path to the destination file.\n\n    Returns:\n        bool: True if the file was copied successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        source_blob = bucket.get_blob(source_file_path)\n\n        if not source_blob:\n            logger.info(f'Source file {source_file_path} not found in bucket {bucket_name}.')\n            return False\n\n        bucket.copy_blob(source_blob, bucket, destination_file_path)\n        logger.info(f'File {source_file_path} copied to {destination_file_path}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to copy file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.delete_file","title":"<code>delete_file(self, bucket_name, file_path)</code>","text":"<p>Deletes a file in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file, including folder(s) and file name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was deleted successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def delete_file(self, bucket_name, file_path):\n    \"\"\"\n    Deletes a file in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file, including folder(s) and file name.\n\n    Returns:\n        bool: True if the file was deleted successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # Get the blob (file) object\n        blob = bucket.blob(file_path)\n\n        # Check if the blob exists\n        if not blob.exists():\n            logger.warning(f'File {file_path} not found in bucket {bucket_name}.')\n            return False\n\n        # Delete the file\n        blob.delete()\n\n        logger.info(f'File {file_path} deleted from bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to delete file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.delete_folder","title":"<code>delete_folder(self, bucket_name, folder_path)</code>","text":"<p>Deletes all files in a folder within a Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>folder_path</code> <code>str</code> <p>The path of the folder to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was deleted successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def delete_folder(self, bucket_name, folder_path):\n    \"\"\"\n    Deletes all files in a folder within a Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        folder_path (str): The path of the folder to delete.\n\n    Returns:\n        bool: True if the folder was deleted successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # List all blobs in the folder\n        blobs = list(bucket.list_blobs(prefix=folder_path))\n\n        # Delete each blob in the folder\n        for blob in blobs:\n            blob.delete()\n            logger.info(f'File {blob.name} deleted.')\n\n        logger.info(f'All files in folder {folder_path} deleted from bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to delete folder: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.download_file","title":"<code>download_file(self, bucket_name, file_path, local_path=None)</code>","text":"<p>Downloads a file from a Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file within the bucket.</p> required <code>local_path</code> <code>str</code> <p>The local path where the file should be saved. If not provided, the file content will be returned as bytes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[bytes, str, None]</code> <p>If local_path is not provided, returns the file content as bytes.     If local_path is provided, returns the path to the saved file.     If some error occurs during the download, returns None.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def download_file(self, bucket_name, file_path, local_path=None):\n    \"\"\"\n    Downloads a file from a Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file within the bucket.\n        local_path (str, optional): The local path where the file should be saved.\n            If not provided, the file content will be returned as bytes.\n\n    Returns:\n        Union[bytes, str, None]: If local_path is not provided, returns the file content as bytes.\n            If local_path is provided, returns the path to the saved file.\n            If some error occurs during the download, returns None.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        blob = bucket.blob(file_path)\n\n        if not blob.exists():\n            logger.error(f\"File {file_path} not found in bucket {bucket_name}\")\n            return None\n\n        if local_path:\n            blob.download_to_filename(local_path)\n            logger.info(f\"File {file_path} downloaded to {local_path}\")\n            return local_path\n        else:\n            content = blob.download_as_bytes()\n            logger.info(f\"File {file_path} downloaded as bytes\")\n            return content\n\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to download file: {str(e)}\")\n        return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.generate_signed_url","title":"<code>generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60)</code>  <code>async</code>","text":"<p>Generates a signed URL for a file in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file, including folder(s) and file name.</p> required <code>expiration_time_minutes</code> <code>int</code> <p>The time in minutes before the URL expires. Defaults to 60 minutes.</p> <code>60</code> <p>Returns:</p> Type Description <code>str</code> <p>The signed URL if generated successfully, None otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>async def generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60):\n    \"\"\"\n    Generates a signed URL for a file in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file, including folder(s) and file name.\n        expiration_time_minutes (int): The time in minutes before the URL expires. Defaults to 60 minutes.\n\n    Returns:\n        str: The signed URL if generated successfully, None otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # Get the blob (file) object\n        blob = bucket.blob(file_path)\n\n        if not blob.exists():\n            logger.warning(f\"File {file_path} does not exist in bucket {bucket_name}\")\n            return None\n\n        # Generate a signed URL for the blob\n        url = blob.generate_signed_url(\n            version=\"v4\",\n            expiration=timedelta(minutes=expiration_time_minutes),\n            method=\"GET\"\n        )\n\n        return url\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to generate signed url: {str(e)}\")\n        return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.get_bucket_metadata","title":"<code>get_bucket_metadata(self, bucket_name)</code>","text":"<p>Retrieves metadata for a specific bucket in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket to retrieve metadata for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the bucket's metadata.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def get_bucket_metadata(self, bucket_name):\n    \"\"\"\n    Retrieves metadata for a specific bucket in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket to retrieve metadata for.\n\n    Returns:\n        dict: A dictionary containing the bucket's metadata.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        metadata = {\n            'id': bucket.id,\n            'name': bucket.name,\n            'location': bucket.location,\n            'storage_class': bucket.storage_class,\n            'created': bucket.time_created,\n            'updated': bucket.updated,\n            'default_event_based_hold': bucket.default_event_based_hold,\n            'retention_period': bucket.retention_period,\n            'labels': bucket.labels,\n            'versioning_enabled': bucket.versioning_enabled,\n            'cors': bucket.cors,\n            'lifecycle_rules': bucket.lifecycle_rules,\n            'logging': bucket.logging,\n            'encryption': bucket.encryption,\n            'owner': bucket.owner,\n            'acl': bucket.acl,\n            'default_acl': bucket.default_object_acl,\n        }\n\n        return metadata\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to get bucket metadata: {str(e)}\")\n        return {}\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.get_file_metadata","title":"<code>get_file_metadata(self, bucket_name, file_path)</code>","text":"<p>Retrieves metadata for a specific file in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file to retrieve metadata for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the file's metadata.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def get_file_metadata(self, bucket_name, file_path):\n    \"\"\"\n    Retrieves metadata for a specific file in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file to retrieve metadata for.\n\n    Returns:\n        dict: A dictionary containing the file's metadata.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        blob = bucket.get_blob(file_path)\n\n        if not blob:\n            logger.info(f'File {file_path} not found in bucket {bucket_name}.')\n            return {}\n\n        metadata = {\n            'name': blob.name,\n            'size': blob.size,\n            'content_type': blob.content_type,\n            'updated': blob.updated,\n            'generation': blob.generation,\n            'metageneration': blob.metageneration,\n            'md5_hash': blob.md5_hash,\n            'crc32c': blob.crc32c,\n            'etag': blob.etag,\n            'public_url': blob.public_url\n        }\n\n        return metadata\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to get file metadata: {str(e)}\")\n        return {}\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.list_buckets","title":"<code>list_buckets(self)</code>","text":"<p>Lists all buckets in the Google Cloud Storage project.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of bucket names.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def list_buckets(self):\n    \"\"\"\n    Lists all buckets in the Google Cloud Storage project.\n\n    Returns:\n        list: A list of bucket names.\n    \"\"\"\n    try:\n        buckets = self.storage_client.list_buckets()\n\n        # Collect bucket names\n        bucket_names = [bucket.name for bucket in buckets]\n\n        return bucket_names\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to list buckets: {str(e)}\")\n        return []\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.list_files","title":"<code>list_files(self, bucket_name, prefix=None)</code>","text":"<p>Lists all files in a bucket or a specific folder in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix (folder path) to list files from. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of file names in the specified bucket or folder.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def list_files(self, bucket_name, prefix=None):\n    \"\"\"\n    Lists all files in a bucket or a specific folder in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        prefix (str, optional): The prefix (folder path) to list files from. Defaults to None.\n\n    Returns:\n        list: A list of file names in the specified bucket or folder.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        # List files in the bucket or a specific folder\n        blobs = bucket.list_blobs(prefix=prefix)\n        file_names = [blob.name for blob in blobs]\n        return file_names\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to list files: {str(e)}\")\n        return []\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.move_file","title":"<code>move_file(self, bucket_name, source_file_path, destination_file_path)</code>","text":"<p>Moves a file from one location to another within the same Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_path</code> <code>str</code> <p>The full path to the source file.</p> required <code>destination_file_path</code> <code>str</code> <p>The full path to the destination file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(bool, str) - (True if the file was moved successfully, False otherwise;                       The public URL of the moved file if successful, None otherwise)</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def move_file(self, bucket_name, source_file_path, destination_file_path):\n    \"\"\"\n    Moves a file from one location to another within the same Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_path (str): The full path to the source file.\n        destination_file_path (str): The full path to the destination file.\n\n    Returns:\n        tuple: (bool, str) - (True if the file was moved successfully, False otherwise;\n                              The public URL of the moved file if successful, None otherwise)\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # Get the blob (file) object for the source file\n        source_blob = bucket.get_blob(source_file_path)\n\n        if not source_blob:\n            logger.error(f'Source file {source_file_path} not found in bucket {bucket_name}.')\n            return False, None\n\n        # Rename (move) the blob\n        new_blob = bucket.rename_blob(source_blob, destination_file_path)\n        logger.info(f'File {source_file_path} moved to {destination_file_path}.')\n\n        return True, new_blob.public_url\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to move file: {str(e)}\")\n        return False, None\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.move_folder","title":"<code>move_folder(self, bucket_name, source_folder, destination_folder)</code>","text":"<p>Moves all files from one folder to another within the same Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_folder</code> <code>str</code> <p>The path of the source folder.</p> required <code>destination_folder</code> <code>str</code> <p>The path of the destination folder.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was moved successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def move_folder(self, bucket_name, source_folder, destination_folder):\n    \"\"\"\n    Moves all files from one folder to another within the same Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_folder (str): The path of the source folder.\n        destination_folder (str): The path of the destination folder.\n\n    Returns:\n        bool: True if the folder was moved successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # List all blobs in the source folder\n        blobs = list(bucket.list_blobs(prefix=source_folder))\n\n        # Move each blob to the destination folder\n        for blob in blobs:\n            new_name = blob.name.replace(source_folder, destination_folder, 1)\n            new_blob = bucket.rename_blob(blob, new_name)\n            logger.info(f'Moved {blob.name} to {new_blob.name}')\n\n        logger.info(f'All files moved from {source_folder} to {destination_folder} in bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to move folder: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.read_file","title":"<code>read_file(self, bucket_name, file_path)</code>","text":"<p>Reads the content of a file from a Google Cloud Storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The path to the file within the bucket.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The content of the file as a string.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there's an error reading the file.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def read_file(self, bucket_name: str, file_path: str) -&gt; str:\n    \"\"\"\n    Reads the content of a file from a Google Cloud Storage bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The path to the file within the bucket.\n\n    Returns:\n        str: The content of the file as a string.\n\n    Raises:\n        Exception: If there's an error reading the file.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        blob = bucket.blob(file_path)\n        content = blob.download_as_text()\n        return content\n    except Exception as e:\n        logger.error(f\"Error reading file from GCS: {str(e)}\")\n        raise Exception(f\"Error reading file from GCS: {str(e)}\")\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.rename_file","title":"<code>rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name)</code>","text":"<p>Renames a file in Google Cloud Storage by copying it to a new name and deleting the original.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_folder</code> <code>str</code> <p>The full path to the folder where the file is located.</p> required <code>source_file_name</code> <code>str</code> <p>The current file name.</p> required <code>new_file_name</code> <code>str</code> <p>The new file name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was renamed successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name):\n    \"\"\"\n    Renames a file in Google Cloud Storage by copying it to a new name and deleting the original.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_folder (str): The full path to the folder where the file is located.\n        source_file_name (str): The current file name.\n        new_file_name (str): The new file name.\n\n    Returns:\n        bool: True if the file was renamed successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        source_path = f\"{source_file_folder}/{source_file_name}\"\n        destination_path = f\"{source_file_folder}/{new_file_name}\"\n\n        source_blob = bucket.get_blob(source_path)\n        if not source_blob:\n            logger.error(f'Source file {source_path} not found in bucket {bucket_name}.')\n            return False\n\n        bucket.copy_blob(source_blob, bucket, destination_path)\n        logger.info(f'File {source_path} copied to {destination_path}.')\n\n        source_blob.delete()\n        logger.info(f'Source file {source_path} deleted.')\n\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to rename file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.set_bucket_permissions","title":"<code>set_bucket_permissions(self, bucket_name, entity, role)</code>","text":"<p>Sets permissions for a bucket in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>entity</code> <code>str</code> <p>The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').</p> required <code>role</code> <code>str</code> <p>The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the permissions were set successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def set_bucket_permissions(self, bucket_name, entity, role):\n    \"\"\"\n    Sets permissions for a bucket in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n        role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n    Returns:\n        bool: True if the permissions were set successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # Get the bucket's ACL\n        acl = bucket.acl\n\n        # Clear existing ACLs for the entity\n        acl.revoke_entity(entity)\n\n        # Add the new permission\n        acl.entity_from_dict({'entity': entity, 'role': role})\n\n        # Save the changes to the ACL\n        acl.save()\n\n        logger.info(f'Permissions for entity {entity} set to {role} on bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to set bucket permissions: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.set_file_permissions","title":"<code>set_file_permissions(self, bucket_name, file_path, entity, role)</code>","text":"<p>Sets permissions for a specific file in Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file within the bucket.</p> required <code>entity</code> <code>str</code> <p>The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').</p> required <code>role</code> <code>str</code> <p>The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the permissions were set successfully, False otherwise.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def set_file_permissions(self, bucket_name, file_path, entity, role):\n    \"\"\"\n    Sets permissions for a specific file in Google Cloud Storage.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file within the bucket.\n        entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n        role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n    Returns:\n        bool: True if the permissions were set successfully, False otherwise.\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        blob = bucket.get_blob(file_path)\n\n        if not blob:\n            logger.error(f'File {file_path} not found in bucket {bucket_name}.')\n            return False\n\n        # Get the blob's ACL\n        acl = blob.acl\n\n        # Clear existing ACLs for the entity\n        acl.revoke_entity(entity)\n\n        # Add the new permission\n        acl.entity_from_dict({'entity': entity, 'role': role})\n\n        # Save the changes to the ACL\n        acl.save()\n\n        logger.info(f'Permissions set successfully on file {file_path} in bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to set file permissions: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.upload_from_local","title":"<code>upload_from_local(self, bucket_name, file_path, destination_blob_name, content_type='application/octet-stream')</code>","text":"<p>Upload a file to the GCS bucket.</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def upload_from_local(self, bucket_name: str, file_path: str, destination_blob_name: str,\n                      content_type='application/octet-stream'):\n    \"\"\"Upload a file to the GCS bucket.\"\"\"\n    try:\n        # Get the bucket from GCS\n        bucket = self.storage_client.get_bucket(bucket_name)\n\n        # Create a blob object with the destination name\n        blob = bucket.blob(destination_blob_name)\n\n        # Upload the file from the local path\n        blob.upload_from_filename(file_path, content_type=content_type)\n\n        logger.info(f\"File {file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n        return blob\n    except Exception as e:\n        logger.error(f\"Error while uploading to GCS: {str(e)}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.gcs_storage.GCSStorage.upload_to_bucket","title":"<code>upload_to_bucket(self, bucket_name, file_stream, destination_blob_name, content_type='application/octet-stream')</code>","text":"<p>Uploads a file to a bucket in Google Cloud Storage using chunked upload.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_stream</code> <code>BytesIO</code> <p>The byte stream of the file to upload.</p> required <code>destination_blob_name</code> <code>str</code> <p>The destination path and file name in the bucket.</p> required <code>content_type</code> <code>str</code> <p>The content type of the file (default: 'application/octet-stream').</p> <code>'application/octet-stream'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(bool, str) - (True if the file was uploaded successfully, False otherwise;                       The public URL of the uploaded file if successful, None otherwise)</p> Source code in <code>kal_utils/storage/gcs_storage.py</code> <pre><code>def upload_to_bucket(self, bucket_name, file_stream, destination_blob_name,\n                     content_type='application/octet-stream'):\n    \"\"\"\n    Uploads a file to a bucket in Google Cloud Storage using chunked upload.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_stream (BytesIO): The byte stream of the file to upload.\n        destination_blob_name (str): The destination path and file name in the bucket.\n        content_type (str): The content type of the file (default: 'application/octet-stream').\n\n    Returns:\n        tuple: (bool, str) - (True if the file was uploaded successfully, False otherwise;\n                              The public URL of the uploaded file if successful, None otherwise)\n    \"\"\"\n    try:\n        bucket = self.storage_client.get_bucket(bucket_name)\n        blob = bucket.blob(destination_blob_name)\n\n        chunk_size = 256 * 1024  # 256 KB chunks\n        file_stream.seek(0, 2)\n        file_size = file_stream.tell()\n        file_stream.seek(0)\n\n        with blob.open('wb') as f:\n            uploaded_bytes = 0\n            while True:\n                chunk = file_stream.read(chunk_size)\n                if not chunk:\n                    break\n                f.write(chunk)\n                uploaded_bytes += len(chunk)\n                progress = (uploaded_bytes / file_size) * 100\n                logger.info(f'Upload progress: {progress:.2f}%')\n\n        blob.content_type = content_type\n        blob.patch()\n\n        logger.info(f'File uploaded to {destination_blob_name} in bucket {bucket_name}.')\n        return True, blob.public_url\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to upload to bucket: {str(e)}\")\n        return False, None\n</code></pre>"},{"location":"storage/#miniostorage","title":"MinIOStorage","text":"Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>class MinIOStorage(BaseStorage):\n    def __init__(self, credentials_json=None):\n        if credentials_json:\n            if os.path.isfile(credentials_json):\n                logger.info(\"Using credentials from file path\")\n                with open(credentials_json, 'r') as f:\n                    minio_creds = json.load(f)\n            else:\n                try:\n                    logger.info(\"Trying to decode base64 credentials\")\n                    decoded_key = base64.b64decode(credentials_json).decode('utf-8')\n                    minio_creds = json.loads(decoded_key)\n                except (base64.binascii.Error, ValueError) as e:\n                    logger.error(\"Failed to decode base64 string, using it as JSON string\")\n                    minio_creds = json.loads(credentials_json)\n\n            self.client = Minio(\n                minio_creds['url'],\n                access_key=minio_creds['accessKey'],\n                secret_key=minio_creds['secretKey'],\n                secure=minio_creds.get('secure', False)\n            )\n            self.url = f\"http://{minio_creds['url']}\"\n        else:\n            self.client = Minio(\n                os.environ.get('MINIO_ENDPOINT'),\n                access_key=os.environ.get('MINIO_ACCESS_KEY'),\n                secret_key=os.environ.get('MINIO_SECRET_KEY'),\n                secure=os.environ.get('MINIO_SECURE', \"false\") == \"true\"\n            )\n            self.url = f\"http://{os.environ.get('MINIO_ENDPOINT')}\"\n\n    def create_bucket(self, bucket_name, location=\"me-west1\", storage_class=\"Standard\"):\n        try:\n            all_chars = string.ascii_letters + string.digits\n            new_name = bucket_name\n            while self.client.bucket_exists(new_name):\n                new_name += random.choice(all_chars)\n\n            self.client.make_bucket(new_name)\n            logger.info(f'Bucket {new_name} created.')\n            return new_name\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to create bucket: {str(e)}\")\n            return None\n\n    def delete_bucket(self, bucket_name):\n        try:\n            self.client.remove_bucket(bucket_name)\n            logger.info(f'Bucket {bucket_name} deleted.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete bucket: {str(e)}\")\n            return False\n\n    def list_files(self, bucket_name, prefix=None):\n        \"\"\"\n        Lists all files in a bucket or a specific folder in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            prefix (str, optional): The prefix (folder path) to list files from. Defaults to None.\n\n        Returns:\n            list: A list of file names in the specified bucket or folder.\n        \"\"\"\n        try:\n            objects = self.client.list_objects(bucket_name, prefix=prefix, recursive=True)\n            file_names = [obj.object_name for obj in objects]\n            return file_names\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to list files: {str(e)}\")\n            return []\n\n    def get_file_metadata(self, bucket_name, file_path):\n        \"\"\"\n        Retrieves metadata for a specific file in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file to retrieve metadata for.\n\n        Returns:\n            dict: A dictionary containing the file's metadata.\n        \"\"\"\n        try:\n            stat = self.client.stat_object(bucket_name, file_path)\n\n            if not stat:\n                logger.info(f'File {file_path} not found in bucket {bucket_name}.')\n                return {}\n\n            metadata = {\n                'name': file_path,\n                'size': stat.size,\n                'content_type': stat.content_type,\n                'updated': stat.last_modified,\n                'etag': stat.etag,\n                'version_id': stat.version_id,\n                'public_url': f\"{self.url}/{bucket_name}/{file_path}\"\n            }\n\n            return metadata\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to get file metadata: {str(e)}\")\n            return {}\n\n    def read_file(self, bucket_name: str, file_path: str) -&gt; str:\n        \"\"\"\n        Reads the content of a file from a MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The path to the file within the bucket.\n\n        Returns:\n            str: The content of the file as a string.\n\n        Raises:\n            Exception: If there's an error reading the file.\n        \"\"\"\n        try:\n            response = self.client.get_object(bucket_name, file_path)\n            content = response.read().decode('utf-8')\n            response.close()\n            response.release_conn()\n            return content\n        except Exception as e:\n            logger.error(f\"Error reading file from MinIO: {str(e)}\")\n            raise Exception(f\"Error reading file from MinIO: {str(e)}\")\n\n    def copy_file(self, bucket_name, source_file_path, destination_file_path):\n        \"\"\"\n        Copies a file from one location to another within the same MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_path (str): The full path to the source file.\n            destination_file_path (str): The full path to the destination file.\n\n        Returns:\n            bool: True if the file was copied successfully, False otherwise.\n        \"\"\"\n        try:\n            source = CopySource(bucket_name, source_file_path)\n\n            result = self.client.copy_object(\n                bucket_name,\n                destination_file_path,\n                source\n            )\n\n            if result.object_name == destination_file_path:\n                logger.info(f'File {source_file_path} copied to {destination_file_path}.')\n                return True\n            else:\n                logger.info(f'Failed to copy file {source_file_path} to {destination_file_path}.')\n                return False\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to copy file: {str(e)}\")\n            return False\n\n    def rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name):\n        \"\"\"\n        Renames a file in MinIO by copying it to a new name and deleting the original.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_folder (str): The full path to the folder where the file is located.\n            source_file_name (str): The current file name.\n            new_file_name (str): The new file name.\n\n        Returns:\n            bool: True if the file was renamed successfully, False otherwise.\n        \"\"\"\n        try:\n            source_path = f\"{source_file_folder}/{source_file_name}\"\n            destination_path = f\"{source_file_folder}/{new_file_name}\"\n\n            # Create a CopySource object for the source file\n            source = CopySource(bucket_name, source_path)\n\n            # Copy the object\n            result = self.client.copy_object(bucket_name, destination_path, source)\n\n            if result.object_name == destination_path:\n                # If copy was successful, remove the original file\n                self.client.remove_object(bucket_name, source_path)\n                logger.info(f'File renamed from {source_path} to {destination_path}.')\n                return True\n            else:\n                logger.info(f'Failed to rename file from {source_path} to {destination_path}.')\n                return False\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to rename file: {str(e)}\")\n            return False\n\n    def get_bucket_metadata(self, bucket_name):\n        \"\"\"\n        Retrieves metadata for a specific bucket in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket to retrieve metadata for.\n\n        Returns:\n            dict: A dictionary containing the bucket's metadata.\n        \"\"\"\n        try:\n            # Check if the bucket exists\n            if not self.client.bucket_exists(bucket_name):\n                logger.error(f\"Bucket {bucket_name} does not exist.\")\n                return {}\n\n            # Get bucket policy\n            policy = self.client.get_bucket_policy(bucket_name)\n\n            # Get bucket versioning\n            versioning = self.client.get_bucket_versioning(bucket_name)\n\n            # Get bucket tags\n            tags = self.client.get_bucket_tags(bucket_name)\n\n            metadata = {\n                'name': bucket_name,\n                'policy': policy,\n                'versioning': versioning,\n                'tags': tags,\n            }\n\n            return metadata\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to get bucket metadata: {str(e)}\")\n            return {}\n\n    def set_bucket_permissions(self, bucket_name, entity, role):\n        \"\"\"\n        Sets permissions for a bucket in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n            role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n        Returns:\n            bool: True if the permissions were set successfully, False otherwise.\n        \"\"\"\n        try:\n            # Map GCS roles to MinIO policy actions\n            role_to_actions = {\n                'READER': ['s3:GetBucketLocation', 's3:ListBucket', 's3:GetObject'],\n                'WRITER': ['s3:GetBucketLocation', 's3:ListBucket', 's3:GetObject', 's3:PutObject', 's3:DeleteObject'],\n                'OWNER': ['s3:*'],\n            }\n\n            if role not in role_to_actions:\n                raise ValueError(f\"Unsupported role: {role}\")\n\n            actions = role_to_actions[role]\n\n            # Create a policy document\n            policy = {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\"AWS\": [entity]},\n                        \"Action\": actions,\n                        \"Resource\": [f\"arn:aws:s3:::{bucket_name}\", f\"arn:aws:s3:::{bucket_name}/*\"]\n                    }\n                ]\n            }\n\n            # Convert the policy to JSON string\n            policy_str = json.dumps(policy)\n\n            # Set the bucket policy\n            self.client.set_bucket_policy(bucket_name, policy_str)\n\n            logger.info(f'Permissions for entity {entity} set to {role} on bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to set bucket permissions: {str(e)}\")\n            return False\n\n    def set_file_permissions(self, bucket_name, file_path, entity, role):\n        \"\"\"\n        Sets permissions for a specific file in MinIO.\n        Note: MinIO doesn't support object-level ACLs directly. This method sets a bucket policy\n        that affects the specified object.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file within the bucket.\n            entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n            role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n        Returns:\n            bool: True if the permissions were set successfully, False otherwise.\n        \"\"\"\n        try:\n            # Map GCS roles to MinIO policy actions\n            role_to_actions = {\n                'READER': ['s3:GetObject'],\n                'WRITER': ['s3:GetObject', 's3:PutObject', 's3:DeleteObject'],\n                'OWNER': ['s3:*'],\n            }\n\n            if role not in role_to_actions:\n                raise ValueError(f\"Unsupported role: {role}\")\n\n            actions = role_to_actions[role]\n\n            # Create a policy document\n            policy = {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\"AWS\": [entity]},\n                        \"Action\": actions,\n                        \"Resource\": [f\"arn:aws:s3:::{bucket_name}/{file_path}\"]\n                    }\n                ]\n            }\n\n            # Convert the policy to JSON string\n            policy_str = json.dumps(policy)\n\n            # Get the current bucket policy\n            current_policy = self.client.get_bucket_policy(bucket_name)\n\n            # Merge the new policy with the existing one\n            # This is a simplistic approach and might need to be more sophisticated\n            # depending on your exact requirements\n            if current_policy:\n                current_policy_json = json.loads(current_policy)\n                current_policy_json['Statement'].append(policy['Statement'][0])\n                policy_str = json.dumps(current_policy_json)\n\n            # Set the updated bucket policy\n            self.client.set_bucket_policy(bucket_name, policy_str)\n\n            logger.info(f'Permissions set successfully on file {file_path} in bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to set file permissions: {str(e)}\")\n            return False\n\n    def upload_to_bucket(self, bucket_name, file_stream, destination_blob_name, content_type='application/octet-stream'):\n        \"\"\"\n        Uploads a file to a bucket in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_stream (BytesIO): The byte stream of the file to upload.\n            destination_blob_name (str): The destination path and file name in the bucket.\n\n        Returns:\n            tuple: (bool, str) - (True if the file was uploaded successfully, False otherwise;\n                                  The public URL of the uploaded file if successful, None otherwise)\n        \"\"\"\n        try:\n            # Get the length of the file stream\n            file_stream.seek(0, 2)  # Go to the end of the stream\n            file_size = file_stream.tell()  # Get the position (size)\n            file_stream.seek(0)  # Go back to the beginning of the stream\n\n            # Upload the file to MinIO\n            self.client.put_object(\n                bucket_name,\n                destination_blob_name,\n                file_stream,\n                file_size,\n                content_type=content_type\n            )\n\n            logger.info(f'File uploaded to {destination_blob_name} in bucket {bucket_name}.')\n\n            # Generate a presigned URL for the uploaded object\n            # Note: This URL will expire after the specified time\n            url = f\"{self.url}/{bucket_name}/{destination_blob_name}\"\n\n            return True, url\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to upload to bucket: {str(e)}\")\n            return False, None\n\n    def upload_from_local(self, bucket_name: str, file_path: str, destination_blob_name: str,\n                          content_type='application/octet-stream'):\n        \"\"\"Upload a file to the MinIO bucket.\"\"\"\n        try:\n            # Get the size of the file to upload\n            file_size = os.stat(file_path).st_size\n\n            # Open the file and upload it using MinIO's `put_object`\n            with open(file_path, 'rb') as file_data:\n                self.client.put_object(\n                    bucket_name=bucket_name,\n                    object_name=destination_blob_name,\n                    data=file_data,\n                    length=file_size,\n                    content_type=content_type\n                )\n\n            logger.info(f\"File {file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error while uploading to MinIO: {str(e)}\", exc_info=True)\n            return False\n\n    def move_folder(self, bucket_name, source_folder, destination_folder):\n        \"\"\"\n        Moves all files from one folder to another within the same MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_folder (str): The path of the source folder.\n            destination_folder (str): The path of the destination folder.\n\n        Returns:\n            bool: True if the folder was moved successfully, False otherwise.\n        \"\"\"\n        try:\n            # List all objects in the source folder\n            objects = self.client.list_objects(bucket_name, prefix=source_folder, recursive=True)\n\n            for obj in objects:\n                # Construct the new object name\n                new_name = obj.object_name.replace(source_folder, destination_folder, 1)\n\n                # Copy the object to the new location\n                result = self.client.copy_object(\n                    bucket_name,\n                    new_name,\n                    CopySource(bucket_name, obj.object_name)\n                )\n\n                # If copy was successful, remove the original object\n                if result:\n                    self.client.remove_object(bucket_name, obj.object_name)\n                    logger.info(f'Moved {obj.object_name} to {new_name}')\n                else:\n                    logger.warning(f'Failed to move {obj.object_name}')\n\n            logger.info(f'All files moved from {source_folder} to {destination_folder} in bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to move folder: {str(e)}\")\n            return False\n\n    def move_file(self, bucket_name, source_file_path, destination_file_path):\n        \"\"\"\n        Moves a file from one location to another within the same MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            source_file_path (str): The full path to the source file.\n            destination_file_path (str): The full path to the destination file.\n\n        Returns:\n            tuple: (bool, str) - (True if the file was moved successfully, False otherwise;\n                                  The URL of the moved file if successful, None otherwise)\n        \"\"\"\n        try:\n            # Copy the object to the new location\n            result = self.client.copy_object(\n                bucket_name,\n                destination_file_path,\n                CopySource(bucket_name, source_file_path)\n            )\n\n            if result:\n                # If copy was successful, remove the original object\n                self.client.remove_object(bucket_name, source_file_path)\n                logger.info(f'File {source_file_path} moved to {destination_file_path}.')\n\n                # Generate a URL for the moved file\n                # Note: This generates a pre-signed URL that will expire\n                url = f\"{self.url}/{bucket_name}/{destination_file_path}\"\n\n                return True, url\n            else:\n                logger.error(f'Failed to move file {source_file_path}.')\n                return False, None\n\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to move file: {str(e)}\")\n            return False, None\n\n    def delete_folder(self, bucket_name, folder_path):\n        \"\"\"\n        Deletes all files in a folder within a MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            folder_path (str): The path of the folder to delete.\n\n        Returns:\n            bool: True if the folder was deleted successfully, False otherwise.\n        \"\"\"\n        try:\n            # List all objects in the folder\n            objects = self.client.list_objects(bucket_name, prefix=folder_path, recursive=True)\n\n            # Delete each object in the folder\n            for obj in objects:\n                self.client.remove_object(bucket_name, obj.object_name)\n                logger.info(f'File {obj.object_name} deleted.')\n\n            logger.info(f'All files in folder {folder_path} deleted from bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete folder: {str(e)}\")\n            return False\n\n    def delete_file(self, bucket_name, file_path):\n        \"\"\"\n        Deletes a file in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file, including folder(s) and file name.\n\n        Returns:\n            bool: True if the file was deleted successfully, False otherwise.\n        \"\"\"\n        try:\n            # Check if the object exists\n            try:\n                self.client.stat_object(bucket_name, file_path)\n            except Exception:\n                logger.warning(f'File {file_path} not found in bucket {bucket_name}.')\n                return False\n\n            # Delete the file\n            self.client.remove_object(bucket_name, file_path)\n\n            logger.info(f'File {file_path} deleted from bucket {bucket_name}.')\n            return True\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to delete file: {str(e)}\")\n            return False\n\n    async def generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60):\n        \"\"\"\n        Generates a presigned URL for a file in MinIO.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file, including folder(s) and file name.\n            expiration_time_minutes (int): The time in minutes before the URL expires. Defaults to 60 minutes.\n\n        Returns:\n            str: The presigned URL if generated successfully, None otherwise.\n        \"\"\"\n        try:\n            # Check if the object exists\n            try:\n                self.client.stat_object(bucket_name, file_path)\n            except Exception:\n                logger.warning(f\"File {file_path} does not exist in bucket {bucket_name}\")\n                return None\n\n            # Generate a presigned URL for the object\n            url = self.client.presigned_get_object(\n                bucket_name,\n                file_path,\n                expires=timedelta(minutes=expiration_time_minutes)\n            )\n\n            return url\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to generate signed url: {str(e)}\")\n            return None\n\n    def list_buckets(self):\n        \"\"\"\n        Lists all buckets in the MinIO instance.\n\n        Returns:\n            list: A list of bucket names.\n        \"\"\"\n        try:\n            buckets = self.client.list_buckets()\n\n            # Collect bucket names\n            bucket_names = [bucket.name for bucket in buckets]\n\n            return bucket_names\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to list buckets: {str(e)}\")\n            return []\n\n\n    def download_file(self, bucket_name, file_path, local_path=None):\n        \"\"\"\n        Downloads a file from a MinIO bucket.\n\n        Args:\n            bucket_name (str): The name of the bucket.\n            file_path (str): The full path to the file within the bucket.\n            local_path (str, optional): The local path where the file should be saved.\n                If not provided, the file content will be returned as bytes.\n\n        Returns:\n            Union[bytes, str, None]: If local_path is not provided, returns the file content as bytes.\n                If local_path is provided, returns the path to the saved file.\n                If some error occurs during the download, returns None.\n        \"\"\"\n        try:\n            # Check if the object exists\n            try:\n                self.client.stat_object(bucket_name, file_path)\n            except Exception:\n                logger.error(f\"File {file_path} not found in bucket {bucket_name}\")\n                return None\n\n            if local_path:\n                self.client.fget_object(bucket_name, file_path, local_path)\n                logger.info(f\"File {file_path} downloaded to {local_path}\")\n                return local_path\n            else:\n                response = self.client.get_object(bucket_name, file_path)\n                content = response.read()\n                response.close()\n                response.release_conn()\n                logger.info(f\"File {file_path} downloaded as bytes\")\n                return content\n\n        except Exception as e:\n            logger.error(f\"Error occurred while trying to download file: {str(e)}\")\n            return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.copy_file","title":"<code>copy_file(self, bucket_name, source_file_path, destination_file_path)</code>","text":"<p>Copies a file from one location to another within the same MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_path</code> <code>str</code> <p>The full path to the source file.</p> required <code>destination_file_path</code> <code>str</code> <p>The full path to the destination file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was copied successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def copy_file(self, bucket_name, source_file_path, destination_file_path):\n    \"\"\"\n    Copies a file from one location to another within the same MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_path (str): The full path to the source file.\n        destination_file_path (str): The full path to the destination file.\n\n    Returns:\n        bool: True if the file was copied successfully, False otherwise.\n    \"\"\"\n    try:\n        source = CopySource(bucket_name, source_file_path)\n\n        result = self.client.copy_object(\n            bucket_name,\n            destination_file_path,\n            source\n        )\n\n        if result.object_name == destination_file_path:\n            logger.info(f'File {source_file_path} copied to {destination_file_path}.')\n            return True\n        else:\n            logger.info(f'Failed to copy file {source_file_path} to {destination_file_path}.')\n            return False\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to copy file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.delete_file","title":"<code>delete_file(self, bucket_name, file_path)</code>","text":"<p>Deletes a file in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file, including folder(s) and file name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was deleted successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def delete_file(self, bucket_name, file_path):\n    \"\"\"\n    Deletes a file in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file, including folder(s) and file name.\n\n    Returns:\n        bool: True if the file was deleted successfully, False otherwise.\n    \"\"\"\n    try:\n        # Check if the object exists\n        try:\n            self.client.stat_object(bucket_name, file_path)\n        except Exception:\n            logger.warning(f'File {file_path} not found in bucket {bucket_name}.')\n            return False\n\n        # Delete the file\n        self.client.remove_object(bucket_name, file_path)\n\n        logger.info(f'File {file_path} deleted from bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to delete file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.delete_folder","title":"<code>delete_folder(self, bucket_name, folder_path)</code>","text":"<p>Deletes all files in a folder within a MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>folder_path</code> <code>str</code> <p>The path of the folder to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was deleted successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def delete_folder(self, bucket_name, folder_path):\n    \"\"\"\n    Deletes all files in a folder within a MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        folder_path (str): The path of the folder to delete.\n\n    Returns:\n        bool: True if the folder was deleted successfully, False otherwise.\n    \"\"\"\n    try:\n        # List all objects in the folder\n        objects = self.client.list_objects(bucket_name, prefix=folder_path, recursive=True)\n\n        # Delete each object in the folder\n        for obj in objects:\n            self.client.remove_object(bucket_name, obj.object_name)\n            logger.info(f'File {obj.object_name} deleted.')\n\n        logger.info(f'All files in folder {folder_path} deleted from bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to delete folder: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.download_file","title":"<code>download_file(self, bucket_name, file_path, local_path=None)</code>","text":"<p>Downloads a file from a MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file within the bucket.</p> required <code>local_path</code> <code>str</code> <p>The local path where the file should be saved. If not provided, the file content will be returned as bytes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[bytes, str, None]</code> <p>If local_path is not provided, returns the file content as bytes.     If local_path is provided, returns the path to the saved file.     If some error occurs during the download, returns None.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def download_file(self, bucket_name, file_path, local_path=None):\n    \"\"\"\n    Downloads a file from a MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file within the bucket.\n        local_path (str, optional): The local path where the file should be saved.\n            If not provided, the file content will be returned as bytes.\n\n    Returns:\n        Union[bytes, str, None]: If local_path is not provided, returns the file content as bytes.\n            If local_path is provided, returns the path to the saved file.\n            If some error occurs during the download, returns None.\n    \"\"\"\n    try:\n        # Check if the object exists\n        try:\n            self.client.stat_object(bucket_name, file_path)\n        except Exception:\n            logger.error(f\"File {file_path} not found in bucket {bucket_name}\")\n            return None\n\n        if local_path:\n            self.client.fget_object(bucket_name, file_path, local_path)\n            logger.info(f\"File {file_path} downloaded to {local_path}\")\n            return local_path\n        else:\n            response = self.client.get_object(bucket_name, file_path)\n            content = response.read()\n            response.close()\n            response.release_conn()\n            logger.info(f\"File {file_path} downloaded as bytes\")\n            return content\n\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to download file: {str(e)}\")\n        return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.generate_signed_url","title":"<code>generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60)</code>  <code>async</code>","text":"<p>Generates a presigned URL for a file in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file, including folder(s) and file name.</p> required <code>expiration_time_minutes</code> <code>int</code> <p>The time in minutes before the URL expires. Defaults to 60 minutes.</p> <code>60</code> <p>Returns:</p> Type Description <code>str</code> <p>The presigned URL if generated successfully, None otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>async def generate_signed_url(self, bucket_name, file_path, expiration_time_minutes=60):\n    \"\"\"\n    Generates a presigned URL for a file in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file, including folder(s) and file name.\n        expiration_time_minutes (int): The time in minutes before the URL expires. Defaults to 60 minutes.\n\n    Returns:\n        str: The presigned URL if generated successfully, None otherwise.\n    \"\"\"\n    try:\n        # Check if the object exists\n        try:\n            self.client.stat_object(bucket_name, file_path)\n        except Exception:\n            logger.warning(f\"File {file_path} does not exist in bucket {bucket_name}\")\n            return None\n\n        # Generate a presigned URL for the object\n        url = self.client.presigned_get_object(\n            bucket_name,\n            file_path,\n            expires=timedelta(minutes=expiration_time_minutes)\n        )\n\n        return url\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to generate signed url: {str(e)}\")\n        return None\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.get_bucket_metadata","title":"<code>get_bucket_metadata(self, bucket_name)</code>","text":"<p>Retrieves metadata for a specific bucket in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket to retrieve metadata for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the bucket's metadata.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def get_bucket_metadata(self, bucket_name):\n    \"\"\"\n    Retrieves metadata for a specific bucket in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket to retrieve metadata for.\n\n    Returns:\n        dict: A dictionary containing the bucket's metadata.\n    \"\"\"\n    try:\n        # Check if the bucket exists\n        if not self.client.bucket_exists(bucket_name):\n            logger.error(f\"Bucket {bucket_name} does not exist.\")\n            return {}\n\n        # Get bucket policy\n        policy = self.client.get_bucket_policy(bucket_name)\n\n        # Get bucket versioning\n        versioning = self.client.get_bucket_versioning(bucket_name)\n\n        # Get bucket tags\n        tags = self.client.get_bucket_tags(bucket_name)\n\n        metadata = {\n            'name': bucket_name,\n            'policy': policy,\n            'versioning': versioning,\n            'tags': tags,\n        }\n\n        return metadata\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to get bucket metadata: {str(e)}\")\n        return {}\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.get_file_metadata","title":"<code>get_file_metadata(self, bucket_name, file_path)</code>","text":"<p>Retrieves metadata for a specific file in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file to retrieve metadata for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the file's metadata.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def get_file_metadata(self, bucket_name, file_path):\n    \"\"\"\n    Retrieves metadata for a specific file in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file to retrieve metadata for.\n\n    Returns:\n        dict: A dictionary containing the file's metadata.\n    \"\"\"\n    try:\n        stat = self.client.stat_object(bucket_name, file_path)\n\n        if not stat:\n            logger.info(f'File {file_path} not found in bucket {bucket_name}.')\n            return {}\n\n        metadata = {\n            'name': file_path,\n            'size': stat.size,\n            'content_type': stat.content_type,\n            'updated': stat.last_modified,\n            'etag': stat.etag,\n            'version_id': stat.version_id,\n            'public_url': f\"{self.url}/{bucket_name}/{file_path}\"\n        }\n\n        return metadata\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to get file metadata: {str(e)}\")\n        return {}\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.list_buckets","title":"<code>list_buckets(self)</code>","text":"<p>Lists all buckets in the MinIO instance.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of bucket names.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def list_buckets(self):\n    \"\"\"\n    Lists all buckets in the MinIO instance.\n\n    Returns:\n        list: A list of bucket names.\n    \"\"\"\n    try:\n        buckets = self.client.list_buckets()\n\n        # Collect bucket names\n        bucket_names = [bucket.name for bucket in buckets]\n\n        return bucket_names\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to list buckets: {str(e)}\")\n        return []\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.list_files","title":"<code>list_files(self, bucket_name, prefix=None)</code>","text":"<p>Lists all files in a bucket or a specific folder in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix (folder path) to list files from. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of file names in the specified bucket or folder.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def list_files(self, bucket_name, prefix=None):\n    \"\"\"\n    Lists all files in a bucket or a specific folder in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        prefix (str, optional): The prefix (folder path) to list files from. Defaults to None.\n\n    Returns:\n        list: A list of file names in the specified bucket or folder.\n    \"\"\"\n    try:\n        objects = self.client.list_objects(bucket_name, prefix=prefix, recursive=True)\n        file_names = [obj.object_name for obj in objects]\n        return file_names\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to list files: {str(e)}\")\n        return []\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.move_file","title":"<code>move_file(self, bucket_name, source_file_path, destination_file_path)</code>","text":"<p>Moves a file from one location to another within the same MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_path</code> <code>str</code> <p>The full path to the source file.</p> required <code>destination_file_path</code> <code>str</code> <p>The full path to the destination file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(bool, str) - (True if the file was moved successfully, False otherwise;                       The URL of the moved file if successful, None otherwise)</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def move_file(self, bucket_name, source_file_path, destination_file_path):\n    \"\"\"\n    Moves a file from one location to another within the same MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_path (str): The full path to the source file.\n        destination_file_path (str): The full path to the destination file.\n\n    Returns:\n        tuple: (bool, str) - (True if the file was moved successfully, False otherwise;\n                              The URL of the moved file if successful, None otherwise)\n    \"\"\"\n    try:\n        # Copy the object to the new location\n        result = self.client.copy_object(\n            bucket_name,\n            destination_file_path,\n            CopySource(bucket_name, source_file_path)\n        )\n\n        if result:\n            # If copy was successful, remove the original object\n            self.client.remove_object(bucket_name, source_file_path)\n            logger.info(f'File {source_file_path} moved to {destination_file_path}.')\n\n            # Generate a URL for the moved file\n            # Note: This generates a pre-signed URL that will expire\n            url = f\"{self.url}/{bucket_name}/{destination_file_path}\"\n\n            return True, url\n        else:\n            logger.error(f'Failed to move file {source_file_path}.')\n            return False, None\n\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to move file: {str(e)}\")\n        return False, None\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.move_folder","title":"<code>move_folder(self, bucket_name, source_folder, destination_folder)</code>","text":"<p>Moves all files from one folder to another within the same MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_folder</code> <code>str</code> <p>The path of the source folder.</p> required <code>destination_folder</code> <code>str</code> <p>The path of the destination folder.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was moved successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def move_folder(self, bucket_name, source_folder, destination_folder):\n    \"\"\"\n    Moves all files from one folder to another within the same MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_folder (str): The path of the source folder.\n        destination_folder (str): The path of the destination folder.\n\n    Returns:\n        bool: True if the folder was moved successfully, False otherwise.\n    \"\"\"\n    try:\n        # List all objects in the source folder\n        objects = self.client.list_objects(bucket_name, prefix=source_folder, recursive=True)\n\n        for obj in objects:\n            # Construct the new object name\n            new_name = obj.object_name.replace(source_folder, destination_folder, 1)\n\n            # Copy the object to the new location\n            result = self.client.copy_object(\n                bucket_name,\n                new_name,\n                CopySource(bucket_name, obj.object_name)\n            )\n\n            # If copy was successful, remove the original object\n            if result:\n                self.client.remove_object(bucket_name, obj.object_name)\n                logger.info(f'Moved {obj.object_name} to {new_name}')\n            else:\n                logger.warning(f'Failed to move {obj.object_name}')\n\n        logger.info(f'All files moved from {source_folder} to {destination_folder} in bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to move folder: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.read_file","title":"<code>read_file(self, bucket_name, file_path)</code>","text":"<p>Reads the content of a file from a MinIO bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The path to the file within the bucket.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The content of the file as a string.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there's an error reading the file.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def read_file(self, bucket_name: str, file_path: str) -&gt; str:\n    \"\"\"\n    Reads the content of a file from a MinIO bucket.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The path to the file within the bucket.\n\n    Returns:\n        str: The content of the file as a string.\n\n    Raises:\n        Exception: If there's an error reading the file.\n    \"\"\"\n    try:\n        response = self.client.get_object(bucket_name, file_path)\n        content = response.read().decode('utf-8')\n        response.close()\n        response.release_conn()\n        return content\n    except Exception as e:\n        logger.error(f\"Error reading file from MinIO: {str(e)}\")\n        raise Exception(f\"Error reading file from MinIO: {str(e)}\")\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.rename_file","title":"<code>rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name)</code>","text":"<p>Renames a file in MinIO by copying it to a new name and deleting the original.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>source_file_folder</code> <code>str</code> <p>The full path to the folder where the file is located.</p> required <code>source_file_name</code> <code>str</code> <p>The current file name.</p> required <code>new_file_name</code> <code>str</code> <p>The new file name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was renamed successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def rename_file(self, bucket_name, source_file_folder, source_file_name, new_file_name):\n    \"\"\"\n    Renames a file in MinIO by copying it to a new name and deleting the original.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        source_file_folder (str): The full path to the folder where the file is located.\n        source_file_name (str): The current file name.\n        new_file_name (str): The new file name.\n\n    Returns:\n        bool: True if the file was renamed successfully, False otherwise.\n    \"\"\"\n    try:\n        source_path = f\"{source_file_folder}/{source_file_name}\"\n        destination_path = f\"{source_file_folder}/{new_file_name}\"\n\n        # Create a CopySource object for the source file\n        source = CopySource(bucket_name, source_path)\n\n        # Copy the object\n        result = self.client.copy_object(bucket_name, destination_path, source)\n\n        if result.object_name == destination_path:\n            # If copy was successful, remove the original file\n            self.client.remove_object(bucket_name, source_path)\n            logger.info(f'File renamed from {source_path} to {destination_path}.')\n            return True\n        else:\n            logger.info(f'Failed to rename file from {source_path} to {destination_path}.')\n            return False\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to rename file: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.set_bucket_permissions","title":"<code>set_bucket_permissions(self, bucket_name, entity, role)</code>","text":"<p>Sets permissions for a bucket in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>entity</code> <code>str</code> <p>The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').</p> required <code>role</code> <code>str</code> <p>The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the permissions were set successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def set_bucket_permissions(self, bucket_name, entity, role):\n    \"\"\"\n    Sets permissions for a bucket in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n        role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n    Returns:\n        bool: True if the permissions were set successfully, False otherwise.\n    \"\"\"\n    try:\n        # Map GCS roles to MinIO policy actions\n        role_to_actions = {\n            'READER': ['s3:GetBucketLocation', 's3:ListBucket', 's3:GetObject'],\n            'WRITER': ['s3:GetBucketLocation', 's3:ListBucket', 's3:GetObject', 's3:PutObject', 's3:DeleteObject'],\n            'OWNER': ['s3:*'],\n        }\n\n        if role not in role_to_actions:\n            raise ValueError(f\"Unsupported role: {role}\")\n\n        actions = role_to_actions[role]\n\n        # Create a policy document\n        policy = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\"AWS\": [entity]},\n                    \"Action\": actions,\n                    \"Resource\": [f\"arn:aws:s3:::{bucket_name}\", f\"arn:aws:s3:::{bucket_name}/*\"]\n                }\n            ]\n        }\n\n        # Convert the policy to JSON string\n        policy_str = json.dumps(policy)\n\n        # Set the bucket policy\n        self.client.set_bucket_policy(bucket_name, policy_str)\n\n        logger.info(f'Permissions for entity {entity} set to {role} on bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to set bucket permissions: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.set_file_permissions","title":"<code>set_file_permissions(self, bucket_name, file_path, entity, role)</code>","text":"<p>Sets permissions for a specific file in MinIO. Note: MinIO doesn't support object-level ACLs directly. This method sets a bucket policy that affects the specified object.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The full path to the file within the bucket.</p> required <code>entity</code> <code>str</code> <p>The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').</p> required <code>role</code> <code>str</code> <p>The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the permissions were set successfully, False otherwise.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def set_file_permissions(self, bucket_name, file_path, entity, role):\n    \"\"\"\n    Sets permissions for a specific file in MinIO.\n    Note: MinIO doesn't support object-level ACLs directly. This method sets a bucket policy\n    that affects the specified object.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_path (str): The full path to the file within the bucket.\n        entity (str): The entity to set permissions for (e.g., 'user-email@example.com', 'group-group@example.com').\n        role (str): The role to assign to the entity (e.g., 'OWNER', 'READER', 'WRITER').\n\n    Returns:\n        bool: True if the permissions were set successfully, False otherwise.\n    \"\"\"\n    try:\n        # Map GCS roles to MinIO policy actions\n        role_to_actions = {\n            'READER': ['s3:GetObject'],\n            'WRITER': ['s3:GetObject', 's3:PutObject', 's3:DeleteObject'],\n            'OWNER': ['s3:*'],\n        }\n\n        if role not in role_to_actions:\n            raise ValueError(f\"Unsupported role: {role}\")\n\n        actions = role_to_actions[role]\n\n        # Create a policy document\n        policy = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\"AWS\": [entity]},\n                    \"Action\": actions,\n                    \"Resource\": [f\"arn:aws:s3:::{bucket_name}/{file_path}\"]\n                }\n            ]\n        }\n\n        # Convert the policy to JSON string\n        policy_str = json.dumps(policy)\n\n        # Get the current bucket policy\n        current_policy = self.client.get_bucket_policy(bucket_name)\n\n        # Merge the new policy with the existing one\n        # This is a simplistic approach and might need to be more sophisticated\n        # depending on your exact requirements\n        if current_policy:\n            current_policy_json = json.loads(current_policy)\n            current_policy_json['Statement'].append(policy['Statement'][0])\n            policy_str = json.dumps(current_policy_json)\n\n        # Set the updated bucket policy\n        self.client.set_bucket_policy(bucket_name, policy_str)\n\n        logger.info(f'Permissions set successfully on file {file_path} in bucket {bucket_name}.')\n        return True\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to set file permissions: {str(e)}\")\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.upload_from_local","title":"<code>upload_from_local(self, bucket_name, file_path, destination_blob_name, content_type='application/octet-stream')</code>","text":"<p>Upload a file to the MinIO bucket.</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def upload_from_local(self, bucket_name: str, file_path: str, destination_blob_name: str,\n                      content_type='application/octet-stream'):\n    \"\"\"Upload a file to the MinIO bucket.\"\"\"\n    try:\n        # Get the size of the file to upload\n        file_size = os.stat(file_path).st_size\n\n        # Open the file and upload it using MinIO's `put_object`\n        with open(file_path, 'rb') as file_data:\n            self.client.put_object(\n                bucket_name=bucket_name,\n                object_name=destination_blob_name,\n                data=file_data,\n                length=file_size,\n                content_type=content_type\n            )\n\n        logger.info(f\"File {file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error while uploading to MinIO: {str(e)}\", exc_info=True)\n        return False\n</code></pre>"},{"location":"storage/#kal_utils.storage.minio_storage.MinIOStorage.upload_to_bucket","title":"<code>upload_to_bucket(self, bucket_name, file_stream, destination_blob_name, content_type='application/octet-stream')</code>","text":"<p>Uploads a file to a bucket in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_stream</code> <code>BytesIO</code> <p>The byte stream of the file to upload.</p> required <code>destination_blob_name</code> <code>str</code> <p>The destination path and file name in the bucket.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(bool, str) - (True if the file was uploaded successfully, False otherwise;                       The public URL of the uploaded file if successful, None otherwise)</p> Source code in <code>kal_utils/storage/minio_storage.py</code> <pre><code>def upload_to_bucket(self, bucket_name, file_stream, destination_blob_name, content_type='application/octet-stream'):\n    \"\"\"\n    Uploads a file to a bucket in MinIO.\n\n    Args:\n        bucket_name (str): The name of the bucket.\n        file_stream (BytesIO): The byte stream of the file to upload.\n        destination_blob_name (str): The destination path and file name in the bucket.\n\n    Returns:\n        tuple: (bool, str) - (True if the file was uploaded successfully, False otherwise;\n                              The public URL of the uploaded file if successful, None otherwise)\n    \"\"\"\n    try:\n        # Get the length of the file stream\n        file_stream.seek(0, 2)  # Go to the end of the stream\n        file_size = file_stream.tell()  # Get the position (size)\n        file_stream.seek(0)  # Go back to the beginning of the stream\n\n        # Upload the file to MinIO\n        self.client.put_object(\n            bucket_name,\n            destination_blob_name,\n            file_stream,\n            file_size,\n            content_type=content_type\n        )\n\n        logger.info(f'File uploaded to {destination_blob_name} in bucket {bucket_name}.')\n\n        # Generate a presigned URL for the uploaded object\n        # Note: This URL will expire after the specified time\n        url = f\"{self.url}/{bucket_name}/{destination_blob_name}\"\n\n        return True, url\n    except Exception as e:\n        logger.error(f\"Error occurred while trying to upload to bucket: {str(e)}\")\n        return False, None\n</code></pre>"},{"location":"storage/#functions","title":"Functions","text":""},{"location":"storage/#get_storage_client","title":"get_storage_client","text":"Source code in <code>kal_utils/storage/__init__.py</code> <pre><code>def get_storage_client(credentials_json = None) -&gt; BaseStorage:\n    storage_type = os.environ.get('STORAGE_TYPE', 'GCS').upper()\n    if storage_type == 'GCS':\n        return GCSStorage(credentials_json)\n    elif storage_type == 'MINIO':\n        return MinIOStorage(credentials_json)\n    else:\n        raise ValueError(f\"Unsupported storage type: {storage_type}\")\n</code></pre>"},{"location":"time_zone/","title":"time_zone module","text":""},{"location":"usage/","title":"Usage","text":"<p>To use kal-utils in a project:</p> <pre><code>import kal_utils\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"}]}